{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve already shown that as the number of deployable application **components in your system grows**, it becomes harder to manage them all. **Google was probably the first company that realized it** needed a much better way of deploying and managing their software components and their infrastructure to scale globally. It’s one of only a few companies in the world that runs hundreds of thousands of servers and has had to deal with managing deployments on such a massive scale. This has forced them to develop solutions for making the development and deployment of thousands of software components manageable and cost-efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding its origins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the years, Google developed an internal system called Borg (and later a new system called Omega), that helped both application developers and system administrators manage those thousands of applications and services. In addition to simplifying the development and management, it also helped them achieve a much higher utilization of their infrastructure, which is important when your organization is that large. When you run hundreds of thousands of machines, even tiny improvements in utilization mean savings in the millions of dollars, so the incentives for developing such a system are clear.\n",
    "\n",
    "After having kept **Borg and Omega secret for a whole decade, in 2014 Google introduced Kubernetes**, an open-source system based on the experience gained through Borg, Omega, and other internal Google systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Kubernetes from the top of a mountain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes is a software system that allows you to **easily deploy and manage containerized applications on top of it**. It relies on the features of Linux containers to run heterogeneous applications without having to know any internal details of these applications and without having to manually deploy these applications on each host. Because these apps run in containers, they don’t affect other apps running on the same server, which is critical when you run applications for completely different organizations on the same hardware. This is of paramount importance for cloud providers, because they strive for the best possible utilization of their hardware while still having to maintain complete isolation of hosted applications.\n",
    "\n",
    "Kubernetes enables you to run your software applications on thousands of computer nodes as if all those nodes were a single, enormous computer. It abstracts away the underlying infrastructure and, by doing so, simplifies development, deployment, and management for both development and the operations teams.\n",
    "\n",
    "Deploying applications through Kubernetes is always the same, whether your cluster contains only a couple of nodes or thousands of them. The size of the cluster makes no difference at all. Additional cluster nodes simply represent an additional amount of resources available to deployed apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the core of what Kubernetes does**\n",
    "\n",
    "Figure 1.8 shows the simplest possible view of a Kubernetes system. The system is composed of a master node and any number of worker nodes. When the developer submits a list of apps to the master, Kubernetes deploys them to the cluster of worker nodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to the developer nor to the system administrator.\n",
    "\n",
    "The developer can specify that certain apps must run together and Kubernetes will deploy them on the same worker node. Others will be spread around the cluster, but they can talk to each other in the same way, regardless of where they’re deployed.\n",
    "\n",
    "**Helping developers focus on the core app features**\n",
    "\n",
    "Kubernetes can be thought of as an operating system for the cluster. It relieves application developers from having to implement certain infrastructure-related services into their apps; instead they rely on Kubernetes to provide these services. This includes things such as service discovery, scaling, load-balancing, self-healing, and even leader election. Application developers can therefore focus on implementing the actual features of the applications and not waste time figuring out how to integrate them with the infrastructure.\n",
    "\n",
    "**Helping ops teams achieve better resource utilization**\n",
    "\n",
    "Kubernetes will run your containerized app somewhere in the cluster, provide information to its components on how to find each other, and keep all of them running. Because your application doesn’t care which node it’s running on, Kubernetes can relocate the app at any time, and by mixing and matching apps, achieve far better resource utilization than is possible with manual scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Understanding the benefits of using Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have Kubernetes deployed on all your servers, the ops team doesn’t need to deal with deploying your apps anymore. Because a containerized application already contains all it needs to run, the system administrators don’t need to install anything to deploy and run the app. On any node where Kubernetes is deployed, Kubernetes can run the app immediately without any help from the sysadmins.\n",
    "\n",
    "**Simplifying application deployment**\n",
    "\n",
    "Because Kubernetes **exposes all its worker nodes as a single deployment platform**, application developers can start deploying **applications on their own and don’t need to know anything about the servers** that make up the cluster.\n",
    "\n",
    "In essence, all the nodes are now a **single bunch of computational resources that are waiting for applications to consume them**. A developer doesn’t usually care what kind of server the application is running on, as long as the server can provide the application with adequate system resources.\n",
    "\n",
    "Certain cases do exist where the developer does care what kind of hardware the application should run on. If the nodes are heterogeneous, you’ll find cases when you want certain apps to run on nodes with certain capabilities and run other apps on others. For example, one of your apps may require being run on a system with SSDs instead of HDDs, while other apps run fine on HDDs. In such cases, you obviously want to ensure that particular app is always scheduled to a node with an SSD.\n",
    "\n",
    "Without using Kubernetes, the sysadmin would select one specific node that has an SSD and deploy the app there. But when using Kubernetes, instead of selecting a specific node where your app should be run, it’s more appropriate to tell Kubernetes to only choose among nodes with an SSD. You’ll learn how to do that in chapter 3.\n",
    "\n",
    "**Achieving better utilization of hardware**\n",
    "\n",
    "By setting up Kubernetes on your servers and using it to run your apps instead of **running them manually, you’ve decoupled your app from the infrastructure**. When you tell Kubernetes to run your application, **you’re letting it choose the most appropriate node to run your application on based on the description of the application’s resource** requirements and the available resources on each node.\n",
    "\n",
    "By using containers and not tying the app down to a specific node in your cluster, you’re allowing the app to freely move around the cluster at any time, so the different app components running on the cluster can be mixed and matched to be packed tightly onto the cluster nodes. This ensures the node’s hardware resources are utilized as best as possible.\n",
    "\n",
    "The ability to move applications around the cluster at any time allows Kubernetes to utilize the infrastructure much better than what you can achieve manually. Humans aren’t good at finding optimal combinations, especially when the number of all possible options is huge, such as when you have many application components and many server nodes they can be deployed on. Computers can obviously perform this work much better and faster than humans.\n",
    "\n",
    "**Health checking and self-healing**\n",
    "\n",
    "Having a system that allows moving an application across the cluster at any time is also valuable in the event of server failures. As your cluster size increases, you’ll deal with failing computer components ever more frequently.\n",
    "\n",
    "Kubernetes monitors your app components and the nodes they run on and automatically reschedules them to other nodes in the event of a node failure. This frees the ops team from having to migrate app components manually and allows the team to immediately focus on fixing the node itself and returning it to the pool of available hardware resources instead of focusing on relocating the app.\n",
    "\n",
    "If your infrastructure has enough spare resources to allow normal system operation even without the failed node, the ops team doesn’t even need to react to the failure immediately, such as at 3 a.m. They can sleep tight and deal with the failed node during regular work hours.\n",
    "\n",
    "**Automatic scaling**\n",
    "\n",
    "Using Kubernetes to manage your deployed applications also **means the ops team doesn’t need to constantly monitor the load of individual applications** to react to sudden load spikes. As previously mentioned, Kubernetes can be told to monitor the resources used by each application and to keep adjusting the number of running instances of each application.\n",
    "\n",
    "If Kubernetes is **running on cloud infrastructure, where adding additional nodes is as easy as requesting them through the cloud provider’s API, Kubernetes can even automatically scale the whole cluster size up** or down based on the needs of the deployed applications.\n",
    "\n",
    "**Simplifying application development**\n",
    "\n",
    "The features described in the previous section mostly benefit the operations team. But what about the developers? Does Kubernetes bring anything to their table? It definitely does.\n",
    "\n",
    "If you turn back to the fact that apps run in the same environment both during development and in production, this has a big effect on when bugs are discovered. We all agree the sooner you discover a bug, the easier it is to fix it, and fixing it requires less work. It’s the developers who do the fixing, so this means less work for them.\n",
    "\n",
    "Then there’s the fact that developers don’t need to implement features that they would usually implement. This includes discovery of services and/or peers in a clustered application. Kubernetes does this instead of the app. Usually, the app only needs to look up certain environment variables or perform a DNS lookup. If that’s not enough, the application can query the Kubernetes API server directly to get that and/or other information. Querying the Kubernetes API server like that can even save developers from having to implement complicated mechanisms such as leader election.\n",
    "\n",
    "As a final example of what Kubernetes brings to the table, you also need to consider the increase in confidence developers will feel knowing that when a new version of their app is going to be rolled out, Kubernetes can automatically detect if the new version is bad and stop its rollout immediately. This increase in confidence usually accelerates the continuous delivery of apps, which benefits the whole organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the architecture of a Kubernetes cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at what a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster is composed of many nodes, which can be split into two types:\n",
    "- The `master node`, which hosts the Kubernetes Control Plane that controls and manages the whole Kubernetes system\n",
    "- `Worker nodes` that run the actual applications you deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img alt=\"\" src=\"https://dpzbhybb2pdcj.cloudfront.net/luksa/Figures/01fig09_alt.jpg\" data-action=\"zoom\" data-zoom-src=\"https://dpzbhybb2pdcj.cloudfront.net/luksa/HighResolutionFigures/figure_1-9.png\" class=\"medium-zoom-image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Control Plane**\n",
    "\n",
    "The Control Plane is what controls the cluster and makes it function. It consists of multiple components that can run on a single master node or be split across multiple nodes and replicated to ensure high availability. These components are\n",
    "- The `Kubernetes API Server`, which you and the other Control Plane components communicate with\n",
    "- The `Scheduler`, which schedules your apps (assigns a worker node to each deployable component of your application)\n",
    "- The `Controller Manager`, which performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures, and so on\n",
    "- `etcd`, a reliable distributed data store that persistently stores the cluster configuration.\n",
    "\n",
    "The components of the Control Plane hold and control the state of the cluster, but they don’t run your applications. This is done by the (worker) nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worker nodes are the machines that run your containerized applications. The task of running, monitoring, and providing services to your applications is done by the following components:\n",
    "- `Docker`, rkt, or another container runtime, which runs your containers\n",
    "- The `Kubelet`, which talks to the API server and manages containers on its node\n",
    "- The `Kubernetes Service Proxy (kube-proxy)`, which load-balances network traffic between application components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll explain all these components in detail in chapter 11. I’m not a fan of explaining how things work before first explaining what something does and teaching people to use it. It’s like learning to drive a car. You don’t want to know what’s under the hood. You first want to learn how to drive it from point A to point B. Only after you learn how to do that do you become interested in how a car makes that possible. After all, knowing what’s under the hood may someday help you get the car moving again after it breaks down and leaves you stranded at the side of the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Kubernetes cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your app packaged inside a container image and made available through Docker Hub, you can deploy it in a Kubernetes cluster instead of running it in Docker directly. But first, you need to set up the cluster itself.\n",
    "\n",
    "Setting up a full-fledged, multi-node Kubernetes cluster isn’t a simple task, especially if you’re not well-versed in Linux and networking administration. A proper Kubernetes install spans multiple physical or virtual machines and requires the networking to be set up properly, so that all the containers running inside the Kubernetes cluster can connect to each other through the same flat networking space.\n",
    "\n",
    "A long list of methods exists for installing a Kubernetes cluster. These methods are described in detail in the documentation at http://kubernetes.io. We’re not going to list all of them here, because the list keeps evolving, but Kubernetes can be run on your local development machine, your own organization’s cluster of machines, on cloud providers providing virtual machines (Google Compute Engine, Amazon EC2, Microsoft Azure, and so on), or by using a managed Kubernetes cluster such as Google Kubernetes Engine (previously known as Google Container Engine).\n",
    "\n",
    "In this chapter, we’ll cover two simple options for getting your hands on a running Kubernetes cluster. You’ll see how to run a single-node Kubernetes cluster on your local machine and how to get access to a hosted cluster running on Google Kubernetes Engine (GKE).\n",
    "\n",
    "A third option, which covers installing a cluster with the kubeadm tool, is explained in appendix B. The instructions there show you how to set up a three-node Kubernetes cluster using virtual machines, but I suggest you try it only after reading the first 11 chapters of the book.\n",
    "\n",
    "Another option is to install Kubernetes on Amazon’s AWS (Amazon Web Services). For this, you can look at the kops tool, which is built on top of kubeadm mentioned in the previous paragraph, and is available at http://github.com/kubernetes/kops. It helps you deploy production-grade, highly available Kubernetes clusters on AWS and will eventually support other platforms as well (Google Kubernetes Engine, VMware, vSphere, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a Kubernetes cluster - Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can set up a new Kubernetes cluster, you need to set up your GKE environment. Because the process may change, I’m not listing the exact instructions here. To get started, please follow the instructions at https://cloud.google.com/container-engine/docs/before-you-begin.\n",
    "\n",
    "Roughly, the whole procedure includes\n",
    "- Signing up for a Google account, in the unlikely case you don’t have one already.\n",
    "- Creating a project in the Google Cloud Platform Console.\n",
    "- Enabling billing. This does require your credit card info, but Google provides a 12-month free trial. And they’re nice enough to not start charging automatically after the free trial is over.)\n",
    "- Enabling the Kubernetes Engine API.\n",
    "- Downloading and installing Google Cloud SDK. (This includes the gcloud command-line tool, which you’ll need to create a Kubernetes cluster.)\n",
    "- Installing the kubectl command-line tool with gcloud components install kubectl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nadzorna plošča -> Create cluster (zonal)\n",
    "- Izberemo pravilno regijo\n",
    "- Verzija: `1.16.13-gke.1`\n",
    "- Dodamo node\n",
    "- Zaženemo cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring cluster access for kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na remote virtualki (jo ustavimo):\n",
    "- EDIT -> Access scopes -> Allow full access to all Cloud APIs\n",
    "- SAVE -> prižgemo virtualko\n",
    "\n",
    "#### Install kubectl using native package management \n",
    "\n",
    "    sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2\n",
    "    curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "    echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\n",
    "    sudo apt-get update\n",
    "    sudo apt-get install -y kubectl\n",
    "   \n",
    "- `gcloud compute ssh docker-icta`\n",
    "- `gcloud container clusters get-credentials icta2 --zone europe-west3-a`\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable kubectl autocompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    echo 'source <(kubectl completion bash)' >>~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se odjavimo in ponovno prijavimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking to see the cluster is up and kubectl can talk to it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify your cluster is working, you can use the kubectl cluster-info command shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl cluster-info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the cluster is up. It shows the URLs of the various Kubernetes components, including the API server and the web console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting an overview of your cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a basic idea of what your cluster looks like and how to interact with it, see figure 2.4. Each node runs Docker, the Kubelet and the kube-proxy. You’ll interact with the cluster through the kubectl command line client, which issues REST requests to the Kubernetes API server running on the master node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking if the cluster is up by listing cluster nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use the kubectl command now to list all the nodes in your cluster, as shown in the following listing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kubectl get command can list all kinds of Kubernetes objects. You’ll use it constantly, but it usually shows only the most basic information for the listed objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP: You can log into one of the nodes with `gcloud compute ssh <node-name>` to explore what’s running on the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieving additional details of an object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see more detailed information about an object, you can use the kubectl describe command, which shows much more:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl describe node gke-kubia-85f6-node-0rrx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I’m omitting the actual output of the describe command, because it’s fairly wide and would be completely unreadable here in the book. The output shows the node’s status, its CPU and memory data, system information, containers running on the node, and much more.\n",
    "\n",
    "In the previous kubectl describe example, you specified the name of the node explicitly, but you could also have performed a simple kubectl describe node without typing the node’s name and it would print out a detailed description of all the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP: Running the describe and get commands without specifying the name of the object comes in handy when only one object of a given type exists, so you don’t waste time typing or copy/pasting the object’s name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we’re talking about reducing keystrokes, let me give you additional advice on how to make working with kubectl much easier, before we move on to running your first app in Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up an alias and command-line completion for kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll use kubectl often. You’ll soon realize that having to type the full command every time is a real pain. Before you continue, take a minute to make your life easier by setting up an alias and tab completion for kubectl.\n",
    "\n",
    "**Creating an alias**\n",
    " \n",
    "Throughout the book, I’ll always be using the full name of the kubectl executable, but you may want to add a short alias such as k, so you won’t have to type kubectl every time. If you haven’t used aliases yet, here’s how you define one. Add the following line to your ~/.bashrc or equivalent file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    alias k=kubectl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocompletition:\n",
    "    \n",
    "    source <(kubectl completion bash | sed s/kubectl/k/g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
