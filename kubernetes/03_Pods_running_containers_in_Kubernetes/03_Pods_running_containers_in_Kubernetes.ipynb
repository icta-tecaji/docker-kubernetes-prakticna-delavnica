{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pods: running containers in Kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kubernetes.io/docs/concepts/workloads/pods/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pod is the basic execution unit of a Kubernetes application–the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents processes running on your Cluster.\n",
    "\n",
    "A Pod encapsulates an application’s container (or, in some cases, multiple containers), storage resources, a unique network IP, and options that govern how the container(s) should run. A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources.\n",
    "\n",
    "Docker is the most common container runtime used in a Kubernetes Pod, but Pods support other container runtimes as well.\n",
    "\n",
    "Pods in a Kubernetes cluster can be used in two main ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul><li><strong>Pods that run a single container</strong>. The “one-container-per-Pod” model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container, and Kubernetes manages the Pods rather than the containers directly.</li><li><p><strong>Pods that run multiple containers that need to work together</strong>. A Pod might encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers might form a single cohesive unit of service–one container serving files from a shared volume to the public, while a separate “sidecar” container refreshes or updates those files. The Pod wraps these containers and storage resources together as a single manageable entity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’ll\n",
    "start **reviewing all types of Kubernetes objects (or resources)** in greater detail, so\n",
    "you’ll understand when, how, and why to use each of them. We’ll start with pods,\n",
    "because they’re the central, most important, concept in Kubernetes. Everything\n",
    "else either manages, exposes, or is used by pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Centralni, najpomembnejši koncept v Kubernetesu\n",
    "- Vse ostalo upravlja, izpostavlja oziroma uporablja stroke\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve already learned that a pod is a **co-located group of containers** and represents\n",
    "the basic building block in Kubernetes. **Instead of deploying containers individually,\n",
    "you always deploy and operate on a pod of containers**. We’re not implying that a pod\n",
    "always includes more than one container—it’s common for pods to contain only a single\n",
    "container. The key thing about pods is that when a pod does contain multiple containers,\n",
    "**all of them are always run on a single worker node** — it never spans multiple\n",
    "worker nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **UNDERSTANDING WHY MULTIPLE CONTAINERS ARE BETTER THAN ONE CONTAINER RUNNING MULTIPLE PROCESSES**  Imagine an app consisting of multiple processes that either communicate through\n",
    "IPC (Inter-Process Communication) or through locally stored files, which requires\n",
    "them to run on the same machine. Because in Kubernetes you always run processes in\n",
    "containers and each container is much like an isolated machine, you may think it\n",
    "makes sense to run multiple processes in a single container, but you shouldn’t do that.\n",
    "**Containers are designed to run only a single process per container** (unless the\n",
    "process itself spawns child processes). If you run multiple unrelated processes in a\n",
    "single container, **it is your responsibility to keep all those processes running, manage\n",
    "their logs, and so on**. For example, you’d have to include a mechanism for automatically\n",
    "restarting individual processes if they crash. Also, all those processes would\n",
    "log to the same standard output, so you’d have a hard time figuring out what process\n",
    "logged what. Therefore, you need to run each process in its own container. That’s how Docker\n",
    "and Kubernetes are meant to be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another indication that containers should only run a single process is the fact that the container runtime only restarts the container when the container’s root process dies. It doesn’t care about any child processes created by this root process. If it spawns child processes, it alone is responsible for keeping all these processes running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you’re not supposed to group multiple processes into a single container, it’s\n",
    "obvious you need another higher-level construct that will allow you to bind containers\n",
    "together and manage them as a single unit. This is the reasoning behind pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pod of containers allows you to run closely related processes together and provide\n",
    "them with (almost) the same environment as if they were all running in a single\n",
    "container, while keeping them somewhat isolated. This way, you get the best of both\n",
    "worlds. You can take advantage of all the features containers provide, while at the\n",
    "same time giving the processes the illusion of running together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all containers of a pod run under the same Network and UTS namespaces\n",
    "(we’re talking about Linux namespaces here), they all share the same hostname and\n",
    "network interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, all containers of a pod run under the same IPC namespace\n",
    "and can communicate through IPC. In the latest Kubernetes and Docker versions, they\n",
    "can also share the same PID namespace, but that feature isn’t enabled by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when it comes to the filesystem, things are a little different. Because most of the\n",
    "**container’s filesystem comes from the container image, by default, the filesystem of\n",
    "each container is fully isolated from other containers**. However, it’s possible to have\n",
    "them share file directories using a Kubernetes concept called a Volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One thing to stress here is that because containers in a pod run in the same Network\n",
    "namespace, they share the same IP address and port space. This means processes running\n",
    "in containers of the same pod need to take care not to bind to the same port\n",
    "numbers or they’ll run into port conflicts. But this only concerns containers in the\n",
    "same pod. Containers of different pods can never run into port conflicts, because\n",
    "each pod has a separate port space. \n",
    "\n",
    "- All the containers in a pod also have the same\n",
    "loopback network interface, so a container can communicate with other containers in\n",
    "the same pod through localhost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCING THE FLAT INTER-POD NETWORK**\n",
    " \n",
    "All pods in a Kubernetes cluster reside in a single flat, shared, network-address space\n",
    "(shown in figure 3.2), which means every pod can access every other pod at the other\n",
    "pod’s IP address. No NAT (Network Address Translation) gateways exist between them.\n",
    "When two pods send network packets between each other, they’ll each see the actual\n",
    "IP address of the other as the source IP in the packet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, communication between pods is always simple. It doesn’t matter if two\n",
    "pods are scheduled onto a single or onto different worker nodes; in both cases the\n",
    "containers inside those pods can communicate with each other across the flat NATless\n",
    "network, much like computers on a local area network (LAN), regardless of the\n",
    "actual inter-node network topology. Like a computer on a LAN, each pod gets its own\n",
    "IP address and is accessible from all other pods through this network established specifically\n",
    "for pods. This is usually achieved through an additional software-defined network\n",
    "layered on top of the actual network.\n",
    "\n",
    "To sum up what’s been covered in this section: pods are logical hosts and behave\n",
    "much like physical hosts or VMs in the non-container world. **Processes running in the\n",
    "same pod are like processes running on the same physical or virtual machine, except\n",
    "that each process is encapsulated in a container.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing containers across pods properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think of pods as separate machines, but where each one hosts only a certain\n",
    "app. Unlike the old days, when we used to cram all sorts of apps onto the same\n",
    "host, we don’t do that with pods. Because pods are relatively lightweight, you can have\n",
    "as many as you need without incurring almost any overhead. **Instead of stuffing everything\n",
    "into a single pod, you should organize apps into multiple pods, where each one\n",
    "contains only tightly related components or processes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Although nothing is stopping you from running both the frontend server and the\n",
    "database in a single pod with two containers, it isn’t the most appropriate way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLITTING MULTI-TIER APPS INTO MULTIPLE PODS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If both the frontend and backend are in the same pod, then both will always be\n",
    "run on the same machine.\n",
    "    - If you have a two-node Kubernetes cluster and only this single\n",
    "pod, you’ll only be using a single worker node and not taking advantage of the\n",
    "computational resources (CPU and memory) you have at your disposal on the second\n",
    "node.\n",
    "    - Splitting the pod into two would allow Kubernetes to schedule the frontend to\n",
    "one node and the backend to the other node, thereby improving the utilization of\n",
    "your infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPLITTING INTO MULTIPLE PODS TO ENABLE INDIVIDUAL SCALING**:\n",
    "Another reason why you shouldn’t put them both into a single pod is scaling. A pod is\n",
    "also the basic unit of scaling. Kubernetes can’t horizontally scale individual containers;\n",
    "instead, it scales whole pods. If your pod consists of a frontend and a backend container,\n",
    "when you scale up the number of instances of the pod to, let’s say, two, you end\n",
    "up with two frontend containers and two backend containers.\n",
    "Usually, frontend components have completely different scaling requirements\n",
    "than the backends, so we tend to scale them individually. Not to mention the fact that\n",
    "backends such as databases are usually much harder to scale compared to (stateless)\n",
    "frontend web servers. If you need to scale a container individually, this is a clear indication\n",
    "that it needs to be deployed in a separate pod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNDERSTANDING WHEN TO USE MULTIPLE CONTAINERS IN A POD**: The main reason to put multiple containers into a single pod is when the application\n",
    "consists of one main process and one or more complementary processes, as shown in\n",
    "figure 3.3.  For example, the main container in a pod could be a web server that serves files from\n",
    "a certain file directory, while an additional container (a sidecar container) periodically\n",
    "downloads content from an external source and stores it in the web server’s\n",
    "directory. In chapter 6 you’ll see that you need to use a Kubernetes Volume that you\n",
    "mount into both containers.\n",
    "\n",
    "Other examples of sidecar containers include log rotators and collectors, data processors,\n",
    "communication adapters, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DECIDING WHEN TO USE MULTIPLE CONTAINERS IN A POD**\n",
    "\n",
    "To recap how containers should be grouped into pods—when deciding whether to\n",
    "put two containers into a single pod or into two separate pods, you always need to ask\n",
    "yourself the following questions:\n",
    "- Do they need to be run together or can they run on different hosts?\n",
    "- Do they represent a single whole or are they independent components?\n",
    "- Must they be scaled together or individually?\n",
    "\n",
    "Basically, you should always gravitate toward running containers in separate pods,\n",
    "unless a specific reason requires them to be part of the same pod. Figure 3.4 will help\n",
    "you memorize this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating pods from YAML or JSON descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a simple YAML descriptor for a pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re going to create a file called `ex01-kubia-manual.yaml`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kubia-manual\n",
    "spec:\n",
    "  containers:\n",
    "  - image: luksa/kubia\n",
    "    name: kubia\n",
    "    ports:\n",
    "    - containerPort: 8080\n",
    "      protocol: TCP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It conforms to the v1 version of the Kubernetes API. The\n",
    "type of resource you’re describing is a pod, with the name kubia-manual. The pod\n",
    "consists of a single container based on the luksa/kubia image. You’ve also given a\n",
    "name to the container and indicated that it’s listening on port 8080."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **SPECIFYING CONTAINER PORTS**: Specifying ports in the pod definition is purely informational. Omitting them has no\n",
    "effect on whether clients can connect to the pod through the port or not. If the container is accepting connections \n",
    "through a port bound to the 0.0.0.0 address, other\n",
    "pods can always connect to it, even if the port isn’t listed in the pod spec explicitly. But\n",
    "it makes sense to define the ports explicitly so that everyone using your cluster can\n",
    "quickly see what ports each pod exposes. Explicitly defining ports also allows you to\n",
    "assign a name to each port, which can come in handy, as you’ll see later in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using kubectl create to create the pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the pod from your YAML file, use the kubectl create command:\n",
    "    \n",
    "    $ kubectl create -f kubia-manual.yaml\n",
    "    \n",
    "The kubectl create -f command is used for creating any resource (not only pods)\n",
    "from a YAML or JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRIEVING THE WHOLE DEFINITION OF A RUNNING POD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the pod, you can ask Kubernetes for the full YAML of the pod. You’ll\n",
    "see it’s similar to the YAML you saw earlier. You’ll learn about the additional fields\n",
    "appearing in the returned definition in the next sections. Go ahead and use the following\n",
    "command to see the full descriptor of the pod:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pod kubia-manual -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re more into JSON, you can also tell kubectl to return JSON instead of YAML\n",
    "like this (this works even if you used YAML to create the pod):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get po kubia-manual -o json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEEING YOUR NEWLY CREATED POD IN THE LIST OF PODS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your pod has been created, but how do you know if it’s running? Let’s list pods to see\n",
    "their statuses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s your kubia-manual pod. Its status shows that it’s running. If you’re like me,\n",
    "you’ll probably want to confirm that’s true by talking to the pod. You’ll do that in a\n",
    "minute. First, you’ll look at the app’s log to check for any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing application logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your little Node.js application logs to the process’s standard output. Containerized\n",
    "applications usually log to the standard output and standard error stream instead of\n",
    "writing their logs to files. This is to allow users to view logs of different applications in\n",
    "a simple, standard way.\n",
    "\n",
    "The container runtime (Docker in your case) redirects those streams to files and\n",
    "allows you to get the container’s log by running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker logs <container id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use ssh to log into the node where your pod is running and retrieve its logs\n",
    "with docker logs, but Kubernetes provides an easier way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RETRIEVING A POD’S LOG WITH KUBECTL LOGS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see your pod’s log (more precisely, the container’s log) you run the following command\n",
    "on your local machine (no need to ssh anywhere):\n",
    "    \n",
    "    kubectl logs kubia-manual\n",
    "\n",
    "You haven’t sent any web requests to your Node.js app, so the log only shows a single\n",
    "log statement about the server starting up. As you can see, retrieving logs of an application\n",
    "running in Kubernetes is incredibly simple if the pod only contains a single\n",
    "container.\n",
    "\n",
    "> NOTE: Container logs are automatically rotated daily and every time the log file\n",
    "reaches 10MB in size. The kubectl logs command only shows the log entries\n",
    "from the last rotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPECIFYING THE CONTAINER NAME WHEN GETTING LOGS OF A MULTI-CONTAINER POD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your pod includes multiple containers, you have to explicitly specify the container\n",
    "name by including the -c <container name> option when running kubectl logs. In\n",
    "your kubia-manual pod, you set the container’s name to kubia, so if additional containers\n",
    "exist in the pod, you’d have to get its logs like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl logs kubia-manual -c kubia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can only retrieve container logs of pods that are still in existence. When\n",
    "a pod is deleted, its logs are also deleted. To make a pod’s logs available even after the\n",
    "pod is deleted, you need to set up centralized, cluster-wide logging, which stores all\n",
    "the logs into a central store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending requests to the pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pod is now running—at least that’s what kubectl get and your app’s log say. But\n",
    "how do you see it in action? In the previous chapter, you used the kubectl expose\n",
    "command to create a service to gain access to the pod externally. You’re not going to\n",
    "do that now, because a whole chapter is dedicated to services, and you have other ways\n",
    "of connecting to a pod for testing and debugging purposes. One of them is through\n",
    "port forwarding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FORWARDING A LOCAL NETWORK PORT TO A PORT IN THE POD**\n",
    "\n",
    "When you want to talk to a specific pod without going through a service (for debugging\n",
    "or other reasons), Kubernetes allows you to configure port forwarding to the\n",
    "pod. This is done through the kubectl port-forward command. The following\n",
    "command will forward your machine’s local port 7000 to port 8080 of your kubiamanual\n",
    "pod:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl port-forward kubia-manual 7000:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The port forwarder is running and you can now connect to your pod through the\n",
    "local port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONNECTING TO THE POD THROUGH THE PORT FORWARDER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a different terminal, you can now use curl to send an HTTP request to your pod\n",
    "through the kubectl port-forward proxy running on localhost:8888:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    curl localhost:7000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3.5 shows an overly simplified view of what happens when you send the request.\n",
    "In reality, a couple of additional components sit between the kubectl process and the\n",
    "pod, but they aren’t relevant right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using port forwarding like this is an effective way to test an individual pod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing pods with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have two pods running in your cluster. When deploying actual\n",
    "applications, most users will end up running many more pods. As the number of\n",
    "pods increases, the need for categorizing them into subsets becomes more and\n",
    "more evident.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with microservices architectures, the number of deployed microservices\n",
    "can easily exceed 20 or more. Those components will probably be replicated\n",
    "(multiple copies of the same component will be deployed) and multiple versions or\n",
    "releases (stable, beta, canary, and so on) will run concurrently. This can lead to hundreds\n",
    "of pods in the system. Without a mechanism for organizing them, you end up\n",
    "with a big, incomprehensible mess, such as the one shown in figure 3.6. The figure\n",
    "shows pods of multiple microservices, with several running multiple replicas, and others\n",
    "running different releases of the same microservice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s evident you need a way of organizing them into smaller groups based on arbitrary\n",
    "criteria, so every developer and system administrator dealing with your system can easily\n",
    "see which pod is which. And you’ll want to operate on every pod belonging to a certain\n",
    "group with a single action instead of having to perform the action for each pod\n",
    "individually.\n",
    "\n",
    "**Organizing pods and all other Kubernetes objects is done through labels.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not\n",
    "only pods, but all other Kubernetes resources. A label is an **arbitrary key-value pair** you\n",
    "attach to a resource, which is then utilized when selecting resources using label selectors\n",
    "(resources are filtered based on whether they include the label specified in the selector).\n",
    "A resource can have more than one label, as long as the keys of those labels are\n",
    "unique within that resource. You usually attach labels to resources when you create\n",
    "them, but you can also add additional labels or even modify the values of existing\n",
    "labels later without having to recreate the resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s turn back to the microservices example from figure 3.6. By adding labels to\n",
    "those pods, you get a much-better-organized system that everyone can easily make\n",
    "sense of. Each pod is labeled with two labels:\n",
    "- app, which specifies which app, component, or microservice the pod belongs to.\n",
    "- rel, which shows whether the application running in the pod is a stable, beta,\n",
    "or a canary release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DEFINITION A canary release is when you deploy a new version of an application\n",
    "next to the stable version, and only let a small fraction of users hit the\n",
    "new version to see how it behaves before rolling it out to all users. This prevents\n",
    "bad releases from being exposed to too many users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding these two labels, you’ve essentially organized your pods into two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every developer or ops person with access to your cluster can now easily see the system’s\n",
    "structure and where each pod fits in by looking at the pod’s labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying labels when creating a pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you’ll see labels in action by creating a new pod with two labels. Create a new file\n",
    "called kubia-manual-with-labels.yaml with the contents of the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kubia-manual-v2\n",
    "  labels:\n",
    "    creation_method: manual\n",
    "    env: prod\n",
    "spec:\n",
    "  containers:\n",
    "   - image: luksa/kubia\n",
    "     name: kubia\n",
    "     ports:\n",
    "     - containerPort: 8080\n",
    "       protocol: TCP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve included the labels creation_method=manual and env=data.labels section.\n",
    "You’ll create this pod now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl create -f kubia-manual-with-labels.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kubectl get pods command doesn’t list any labels by default, but you can see\n",
    "them by using the --show-labels switch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pod --show-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of listing all labels, if you’re only interested in certain labels, you can specify\n",
    "them with the -L switch and have each displayed in its own column. List pods again\n",
    "and show the columns for the two labels you’ve attached to your kubia-manual-v2 pod:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pod -L creation_method,env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying labels of existing pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels can also be added to and modified on existing pods. Because the kubia-manual\n",
    "pod was also created manually, let’s add the creation_method=manual label to it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl label pod kubia-manual creation_method=manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,\n",
    "to see how existing labels can be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE You need to use the --overwrite option when changing existing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl label po kubia-manual-v2 env=debug --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the pods again to see the updated labels:\n",
    "    \n",
    "    kubectl get po -L creation_method,env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, attaching labels to resources is trivial, and so is changing them on\n",
    "existing resources. It may not be evident right now, but this is an incredibly powerful\n",
    "feature, as you’ll see in the next chapter. But first, let’s see what you can do with these\n",
    "labels, in addition to displaying them when listing pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing subsets of pods through label selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching labels to resources so you can see the labels next to each resource when listing\n",
    "them isn’t that interesting. But labels go hand in hand with label selectors. Label\n",
    "selectors allow you to select a subset of pods tagged with certain labels and perform an\n",
    "operation on those pods. A label selector is a criterion, which filters resources based\n",
    "on whether they include a certain label with a certain value.\n",
    "A label selector can select resources based on whether the resource\n",
    "- Contains (or doesn’t contain) a label with a certain key\n",
    "- Contains a label with a certain key and value\n",
    "- Contains a label with a certain key, but with a value not equal to the one you\n",
    "specify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing pods using a label selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use label selectors on the pods you’ve created so far. To see all pods you created\n",
    "manually (you labeled them with creation_method=manual), do the following:\n",
    "\n",
    "    kubectl get po -l creation_method=manual\n",
    "    \n",
    "To list all pods that include the env label, whatever its value is:\n",
    "\n",
    "    kubectl get po -l env\n",
    "\n",
    "And those that don’t have the env label:\n",
    "\n",
    "    kubectl get po -l '!env'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE Make sure to use single quotes around !env, so the bash shell doesn’t\n",
    "evaluate the exclamation mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using labels and selectors to constrain pod scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the pods you’ve created so far have been scheduled pretty much randomly across\n",
    "your worker nodes. As I’ve mentioned in the previous chapter, this is the proper way\n",
    "of working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the\n",
    "cluster as a single, large deployment platform, it shouldn’t matter to you what node a\n",
    "pod is scheduled to. Because each pod gets the exact amount of computational\n",
    "resources it requests (CPU, memory, and so on) and its accessibility from other pods\n",
    "isn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any\n",
    "need for you to tell Kubernetes exactly where to schedule your pods.\n",
    "\n",
    "Certain cases exist, however, where you’ll want to have at least a little say in where\n",
    "a pod should be scheduled. A good example is when your hardware infrastructure\n",
    "isn’t homogenous. If part of your worker nodes have spinning hard drives, whereas\n",
    "others have SSDs, you may want to schedule certain pods to one group of nodes and\n",
    "the rest to the other. Another example is when you need to schedule pods performing\n",
    "intensive GPU-based computation only to nodes that provide the required GPU\n",
    "acceleration.\n",
    "\n",
    "You never want to say specifically what node a pod should be scheduled to, because\n",
    "that would couple the application to the infrastructure, whereas the whole idea of\n",
    "Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you\n",
    "want to have a say in where a pod should be scheduled, instead of specifying an exact\n",
    "node, you should describe the node requirements and then let Kubernetes select a\n",
    "node that matches those requirements. This can be done through node labels and\n",
    "node label selectors.\n",
    "\n",
    "### Scheduling to one specific node\n",
    "\n",
    "Similarly, you could also schedule a pod to an exact node, because each node also has\n",
    "a unique label with the key kubernetes.io/hostname and value set to the actual hostname\n",
    "of the node. But setting the nodeSelector to a specific node by the hostname\n",
    "label may lead to the pod being unschedulable if the node is offline. You shouldn’t\n",
    "think in terms of individual nodes. Always think about logical groups of nodes that satisfy\n",
    "certain criteria specified through label selectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping and removing pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting a pod by name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, delete the kubia-gpu pod by name:\n",
    "    \n",
    "    kubectl delete po kubia-gpu\n",
    "\n",
    "\n",
    "By deleting a pod, you’re instructing Kubernetes to terminate all the containers that are\n",
    "part of that pod. Kubernetes sends a SIGTERM signal to the process and waits a certain\n",
    "number of seconds (30 by default) for it to shut down gracefully. If it doesn’t shut down\n",
    "in time, the process is then killed through SIGKILL. To make sure your processes are\n",
    "always shut down gracefully, they need to handle the SIGTERM signal properly.\n",
    "\n",
    "> TIP You can also delete more than one pod by specifying multiple, space-separated\n",
    "names (for example, kubectl delete po pod1 pod2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting pods using label selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying each pod to delete by name, you’ll now use what you’ve learned\n",
    "about label selectors to stop both the kubia-manual and the kubia-manual-v2 pod.\n",
    "Both pods include the creation_method=manual label, so you can delete them by\n",
    "using a label selector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl delete po -l creation_method=manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting (almost) all resources in a namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can delete the ReplicationController and the pods, as well as all the Services\n",
    "you’ve created, by deleting all resources in the current namespace with a single\n",
    "command:\n",
    "    \n",
    "    kubectl delete all --all\n",
    "\n",
    "The first all in the command specifies that you’re deleting resources of all types, and\n",
    "the --all option specifies that you’re deleting all resource instances instead of specifying\n",
    "them by name (you already used this option when you ran the previous delete\n",
    "command).\n",
    "\n",
    "> NOTE Deleting everything with the all keyword doesn’t delete absolutely\n",
    "everything. Certain resources (like Secrets, which we’ll introduce in chapter 7)\n",
    "are preserved and need to be deleted explicitly.\n",
    "\n",
    "As it deletes resources, kubectl will print the name of every resource it deletes. In the\n",
    "list, you should see the kubia ReplicationController and the kubia-http Service you\n",
    "created in chapter 2.\n",
    "\n",
    "> NOTE The kubectl delete all --all command also deletes the kubernetes\n",
    "Service, but it should be recreated automatically in a few moments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
