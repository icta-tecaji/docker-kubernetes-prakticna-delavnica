{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication and other controllers: deploying managed pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you’ve learned so far, pods represent the basic deployable unit in Kubernetes.\n",
    "You know how to create, supervise, and manage them manually. But in real-world\n",
    "use cases, you want your deployments to stay up and running automatically and\n",
    "remain healthy without any manual intervention. To do this, you almost never create\n",
    "pods directly. Instead, you create other types of resources, such as Replication-\n",
    "Controllers or Deployments, which then create and manage the actual pods.\n",
    "\n",
    "When you create unmanaged pods (such as the ones you created in the previous\n",
    "chapter), a cluster node is selected to run the pod and then its containers are\n",
    "run on that node. In this chapter, you’ll learn that Kubernetes then monitors\n",
    "those containers and automatically restarts them if they fail. But if the whole node\n",
    "fails, the pods on the node are lost and will not be replaced with new ones, unless\n",
    "those pods are managed by the previously mentioned ReplicationControllers or similar.\n",
    "In this chapter, you’ll learn how Kubernetes checks if a container is still alive and\n",
    "restarts it if it isn’t. You’ll also learn how to run managed pods—both those that run\n",
    "indefinitely and those that perform a single task and then stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping pods healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main benefits of using Kubernetes is the ability to give it a list of containers\n",
    "and let it keep those containers running somewhere in the cluster. You do this by\n",
    "creating a Pod resource and letting Kubernetes pick a worker node for it and run\n",
    "the pod’s containers on that node. But what if one of those containers dies? What if\n",
    "all containers of a pod die?\n",
    "\n",
    "As soon as a pod is scheduled to a node, the Kubelet on that node will run its containers\n",
    "and, from then on, keep them running as long as the pod exists. If the container’s\n",
    "main process crashes, the Kubelet will restart the container. If your\n",
    "application has a bug that causes it to crash every once in a while, Kubernetes will\n",
    "restart it automatically, so even without doing anything special in the app itself, running\n",
    "the app in Kubernetes automatically gives it the ability to heal itself.\n",
    "\n",
    "But sometimes apps stop working without their process crashing. For example, a\n",
    "Java app with a memory leak will start throwing OutOfMemoryErrors, but the JVM\n",
    "process will keep running. It would be great to have a way for an app to signal to\n",
    "Kubernetes that it’s no longer functioning properly and have Kubernetes restart it.\n",
    "We’ve said that a container that crashes is restarted automatically, so maybe you’re\n",
    "thinking you could catch these types of errors in the app and exit the process when\n",
    "they occur. You can certainly do that, but it still doesn’t solve all your problems.\n",
    "For example, what about those situations when your app stops responding because\n",
    "it falls into an infinite loop or a deadlock? To make sure applications are restarted in\n",
    "such cases, you must check an application’s health from the outside and not depend\n",
    "on the app doing it internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes can probe a container using one of the three mechanisms:\n",
    "- An `HTTP GET probe` performs an HTTP GET request on the container’s IP\n",
    "address, a port and path you specify. If the probe receives a response, and the\n",
    "response code doesn’t represent an error (in other words, if the HTTP response\n",
    "code is 2xx or 3xx), the probe is considered successful. If the server returns an\n",
    "error response code or if it doesn’t respond at all, the probe is considered a failure\n",
    "and the container will be restarted as a result.\n",
    "- A `TCP Socket probe` tries to open a TCP connection to the specified port of the\n",
    "container. If the connection is established successfully, the probe is successful.\n",
    "Otherwise, the container is restarted.\n",
    "- An `Exec probe` executes an arbitrary command inside the container and checks\n",
    "the command’s exit status code. If the status code is 0, the probe is successful.\n",
    "All other codes are considered failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an HTTP-based liveness probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how to add a liveness probe to your Node.js app. Because it’s a web app, it\n",
    "makes sense to add a liveness probe that will check whether its web server is serving\n",
    "requests. But because this particular Node.js app is too simple to ever fail, you’ll need\n",
    "to make the app fail artificially.\n",
    "\n",
    "To properly demo liveness probes, you’ll modify the app slightly and make it\n",
    "return a 500 Internal Server Error HTTP status code for each request after the fifth\n",
    "one—your app will handle the first five client requests properly and then return an\n",
    "error on every subsequent request. Thanks to the liveness probe, it should be restarted\n",
    "when that happens, allowing it to properly handle client requests again.\n",
    "You can find the code of the new app in the book’s code archive (in the folder\n",
    "Chapter04/kubia-unhealthy). I’ve pushed the container image to Docker Hub, so you\n",
    "don’t need to build it yourself.\n",
    "\n",
    "You’ll create a new pod that includes an HTTP GET liveness probe. The following\n",
    "listing shows the YAML for the pod."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kubia-liveness\n",
    "spec:\n",
    "  containers:\n",
    "  - image: luksa/kubia-unhealthy\n",
    "    name: kubia\n",
    "    livenessProbe:\n",
    "      httpGet:\n",
    "        path: /\n",
    "        port: 8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl create -f kubia-liveness-probe.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "periodically\n",
    "perform HTTP GET requests on path / on port 8080 to determine if the container\n",
    "is still healthy. These requests start as soon as the container is run.\n",
    "\n",
    "After five such requests (or actual client requests), your app starts returning\n",
    "HTTP status code 500, which Kubernetes will treat as a probe failure, and will thus\n",
    "restart the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing a liveness probe in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the liveness probe does, try creating the pod now. After about a minute and\n",
    "a half, the container will be restarted. You can see that by running kubectl get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get po kubia-liveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RESTARTS column shows that the pod’s container has been restarted once (if you\n",
    "wait another minute and a half, it gets restarted again, and then the cycle continues\n",
    "indefinitely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see why the container had to be restarted by looking at what kubectl describe\n",
    "prints out, as shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl describe po kubia-liveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the container is currently running, but it previously terminated\n",
    "because of an error. The exit code was 137, which has a special meaning—it denotes\n",
    "that the process was terminated by an external signal. The number 137 is a sum of two\n",
    "numbers: 128+x, where x is the signal number sent to the process that caused it to terminate.\n",
    "In the example, x equals 9, which is the number of the SIGKILL signal, meaning\n",
    "the process was killed forcibly.\n",
    "\n",
    "The events listed at the bottom show why the container was killed—Kubernetes\n",
    "detected the container was unhealthy, so it killed and re-created it.\n",
    "\n",
    "> NOTE When a container is killed, a completely new container is created—it’s\n",
    "not the same container being restarted again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring additional properties of the liveness probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that kubectl describe also displays additional information\n",
    "about the liveness probe:\n",
    "    \n",
    "    Liveness: http-get http://:8080/ delay=0s timeout=1s period=10s #success=1\n",
    "    ➥ #failure=3\n",
    "\n",
    "Beside the liveness probe options you specified explicitly, you can also see additional\n",
    "properties, such as delay, timeout, period, and so on. The delay=0s part shows that\n",
    "the probing begins immediately after the container is started. The timeout is set to\n",
    "only 1 second, so the container must return a response in 1 second or the probe is\n",
    "counted as failed. The container is probed every 10 seconds (period=10s) and the\n",
    "container is restarted after the probe fails three consecutive times (#failure=3).\n",
    "\n",
    "    \n",
    "These additional parameters can be customized when defining the probe. For\n",
    "example, to set the initial delay, add the initialDelaySeconds property to the liveness\n",
    "probe as shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubia-liveness-probe-initial-delay.yaml\n",
    "```yml\n",
    "livenessProbe:\n",
    "  httpGet:\n",
    "    path: /\n",
    "    port: 8080\n",
    "  initialDelaySeconds: 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don’t set the initial delay, the prober will start probing the container as soon as\n",
    "it starts, which usually leads to the probe failing, because the app isn’t ready to start\n",
    "receiving requests. If the number of failures exceeds the failure threshold, the container\n",
    "is restarted before it’s even able to start responding to requests properly.\n",
    "\n",
    "> TIP Always remember to set an initial delay to account for your app’s startup\n",
    "time.\n",
    "\n",
    "I’ve seen this on many occasions and users were confused why their container was\n",
    "being restarted. But if they’d used kubectl describe, they’d have seen that the container\n",
    "terminated with exit code 137 or 143, telling them that the pod was terminated\n",
    "externally. Additionally, the listing of the pod’s events would show that the container\n",
    "was killed because of a failed liveness probe. If you see this happening at pod startup,\n",
    "it’s because you failed to set initialDelaySeconds appropriately.\n",
    "\n",
    "> NOTE Exit code 137 signals that the process was killed by an external signal\n",
    "(exit code is 128 + 9 (SIGKILL). Likewise, exit code 143 corresponds to 128 +\n",
    "15 (SIGTERM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating effective liveness probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pods running in production, you should always define a liveness probe. Without\n",
    "one, Kubernetes has no way of knowing whether your app is still alive or not. As long\n",
    "as the process is still running, Kubernetes will consider the container to be healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT A LIVENESS PROBE SHOULD CHECK**\n",
    "\n",
    "Your simplistic liveness probe simply checks if the server is responding. While this may\n",
    "seem overly simple, even a liveness probe like this does wonders, because it causes the\n",
    "container to be restarted if the web server running within the container stops\n",
    "responding to HTTP requests. Compared to having no liveness probe, this is a major\n",
    "improvement, and may be sufficient in most cases.\n",
    "\n",
    "But for a better liveness check, you’d configure the probe to perform requests on a\n",
    "specific URL path (/health, for example) and have the app perform an internal status\n",
    "check of all the vital components running inside the app to ensure none of them\n",
    "has died or is unresponsive.\n",
    "\n",
    "> TIP Make sure the /health HTTP endpoint doesn’t require authentication;\n",
    "otherwise the probe will always fail, causing your container to be restarted\n",
    "indefinitely.\n",
    "\n",
    "Be sure to check only the internals of the app and nothing influenced by an external\n",
    "factor. For example, a frontend web server’s liveness probe shouldn’t return a failure\n",
    "when the server can’t connect to the backend database. If the underlying cause is in\n",
    "the database itself, restarting the web server container will not fix the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the liveness probe will fail again, you’ll end up with the container restarting\n",
    "repeatedly until the database becomes accessible again.\n",
    "\n",
    "**KEEPING PROBES LIGHT**\n",
    "\n",
    "Liveness probes shouldn’t use too many computational resources and shouldn’t take\n",
    "too long to complete. By default, the probes are executed relatively often and are\n",
    "only allowed one second to complete. Having a probe that does heavy lifting can slow\n",
    "down your container considerably. Later in the book, you’ll also learn about how to\n",
    "limit CPU time available to a container. The probe’s CPU time is counted in the container’s\n",
    "CPU time quota, so having a heavyweight liveness probe will reduce the CPU\n",
    "time available to the main application processes.\n",
    "\n",
    "TIP If you’re running a Java app in your container, be sure to use an HTTP\n",
    "GET liveness probe instead of an Exec probe, where you spin up a whole new\n",
    "JVM to get the liveness information. The same goes for any JVM-based or similar\n",
    "applications, whose start-up procedure requires considerable computational\n",
    "resources.\n",
    "\n",
    "**DON’T BOTHER IMPLEMENTING RETRY LOOPS IN YOUR PROBES**\n",
    "\n",
    "You’ve already seen that the failure threshold for the probe is configurable and usually\n",
    "the probe must fail multiple times before the container is killed. But even if you\n",
    "set the failure threshold to 1, Kubernetes will retry the probe several times before considering\n",
    "it a single failed attempt. Therefore, implementing your own retry loop into\n",
    "the probe is wasted effort.\n",
    "\n",
    "**LIVENESS PROBE WRAP-UP**\n",
    "\n",
    "You now understand that Kubernetes keeps your containers running by restarting\n",
    "them if they crash or if their liveness probes fail. This job is performed by the Kubelet\n",
    "on the node hosting the pod—the Kubernetes Control Plane components running on\n",
    "the master(s) have no part in this process.\n",
    "\n",
    "But if the node itself crashes, it’s the Control Plane that must create replacements for\n",
    "all the pods that went down with the node. It doesn’t do that for pods that you create\n",
    "directly. Those pods aren’t managed by anything except by the Kubelet, but because the\n",
    "Kubelet runs on the node itself, it can’t do anything if the node fails.\n",
    "\n",
    "To make sure your app is restarted on another node, you need to have the pod\n",
    "managed by a ReplicationController or similar mechanism, which we’ll discuss in the\n",
    "rest of this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplicationControllers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ReplicationController is a Kubernetes resource that ensures its pods are always\n",
    "kept running. If the pod disappears for any reason, such as in the event of a node\n",
    "disappearing from the cluster or because the pod was evicted from the node, the\n",
    "ReplicationController notices the missing pod and creates a replacement pod.\n",
    "Figure 4.1 shows what happens when a node goes down and takes two pods with it.\n",
    "\n",
    "Pod A was created directly and is therefore an unmanaged pod, while pod B is managed\n",
    "by a ReplicationController. After the node fails, the ReplicationController creates a\n",
    "new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—\n",
    "nothing will ever recreate it.\n",
    "\n",
    "The ReplicationController in the figure manages only a single pod, but Replication-\n",
    "Controllers, in general, are meant to create and manage multiple copies (replicas) of a\n",
    "pod. That’s where ReplicationControllers got their name from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The operation of a ReplicationController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many things in Kubernetes, a ReplicationController, although an incredibly simple\n",
    "concept, provides or enables the following powerful features:\n",
    "- It makes sure a pod (or multiple pod replicas) is always running by starting a\n",
    "new pod when an existing one goes missing.\n",
    "- When a cluster node fails, it creates replacement replicas for all the pods that\n",
    "were running on the failed node (those that were under the Replication-\n",
    "Controller’s control).\n",
    "- It enables easy horizontal scaling of pods—both manual and automatic (see\n",
    "horizontal pod auto-scaling in chapter 15).\n",
    "\n",
    "> NOTE A pod instance is never relocated to another node. Instead, the\n",
    "ReplicationController creates a completely new pod instance that has no relation\n",
    "to the instance it’s replacing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a ReplicationController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at how to create a ReplicationController and then see how it keeps your\n",
    "pods running. Like pods and other Kubernetes resources, you create a Replication-\n",
    "Controller by posting a JSON or YAML descriptor to the Kubernetes API server.\n",
    "\n",
    "You’re going to create a YAML file called kubia-rc.yaml for your Replication-\n",
    "Controller, as shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: ReplicationController\n",
    "metadata:\n",
    "  name: kubia\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    app: kubia\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: kubia\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: kubia\n",
    "        image: luksa/kubia\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you post the file to the API server, Kubernetes creates a new Replication-\n",
    "Controller named kubia, which makes sure three pod instances always match the\n",
    "label selector app=kubia. When there aren’t enough pods, new pods will be created\n",
    "from the provided pod template. The contents of the template are almost identical to\n",
    "the pod definition you created in the previous chapter.\n",
    "\n",
    "The pod labels in the template must obviously match the label selector of the\n",
    "ReplicationController; otherwise the controller would create new pods indefinitely,\n",
    "because spinning up a new pod wouldn’t bring the actual replica count any closer to\n",
    "the desired number of replicas. To prevent such scenarios, the API server verifies the\n",
    "ReplicationController definition and will not accept it if it’s misconfigured.\n",
    "Not specifying the selector at all is also an option. In that case, it will be configured\n",
    "automatically from the labels in the pod template.\n",
    "\n",
    "> IP Don’t specify a pod selector when defining a ReplicationController. Let\n",
    "Kubernetes extract it from the pod template. This will keep your YAML\n",
    "shorter and simpler.\n",
    "\n",
    "To create the ReplicationController, use the kubectl create command, which you\n",
    "already know:\n",
    "\n",
    "    $ kubectl apply  -f kubia-rc.yaml\n",
    "\n",
    "    \n",
    "As soon as the ReplicationController is created, it goes to work. Let’s see what\n",
    "it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing the ReplicationController in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because no pods exist with the app=kubia label, the ReplicationController should\n",
    "spin up three new pods from the pod template. List the pods to see if the Replication-\n",
    "Controller has done what it’s supposed to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it has! You wanted three pods, and it created three pods. It’s now managing\n",
    "those three pods. Next you’ll mess with them a little to see how the Replication-\n",
    "Controller responds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEEING THE REPLICATIONCONTROLLER RESPOND TO A DELETED POD**\n",
    "\n",
    "First, you’ll delete one of the pods manually to see how the ReplicationController spins\n",
    "up a new one immediately, bringing the number of matching pods back to three:\n",
    "\n",
    "    kubectl delete pod kubia-53thy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the pods again shows four of them, because the one you deleted is terminating,\n",
    "and a new pod has already been created:\n",
    "    \n",
    "    kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReplicationController has done its job again. It’s a nice little helper, isn’t it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GETTING INFORMATION ABOUT A REPLICATIONCONTROLLER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s see what information the kubectl get command shows for Replication-Controllers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE We’re using rc as a shorthand for replicationcontroller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see three columns showing the desired number of pods, the actual number of\n",
    "pods, and how many of them are ready (you’ll learn what that means in the next chapter,\n",
    "when we talk about readiness probes).\n",
    "\n",
    "You can see additional information about your ReplicationController with the\n",
    "kubectl describe command, as shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl describe rc kubia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The list of events at the bottom shows the actions taken by the Replication-\n",
    "Controller—it has created four pods so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESPONDING TO A NODE FAILURE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing the ReplicationController respond to the manual deletion of a pod isn’t too\n",
    "interesting, so let’s look at a better example. If you’re using Google Kubernetes Engine\n",
    "to run these examples, you have a three-node Kubernetes cluster. You’re going to disconnect\n",
    "one of the nodes from the network to simulate a node failure.\n",
    "\n",
    "> NOTE If you’re using Minikube, you can’t do this exercise, because you only\n",
    "have one node that acts both as a master and a worker node.\n",
    "\n",
    "If a node fails in the non-Kubernetes world, the ops team would need to migrate the\n",
    "applications running on that node to other machines manually. Kubernetes, on the\n",
    "other hand, does that automatically. Soon after the ReplicationController detects that\n",
    "its pods are down, it will spin up new pods to replace them.\n",
    "\n",
    "Let’s see this in action. You need to ssh into one of the nodes with the gcloud\n",
    "compute ssh command and then shut down its network interface with sudo ifconfig\n",
    "eth0 down, as shown in the following listing.\n",
    "\n",
    "> NOTE Choose a node that runs at least one of your pods by listing pods with\n",
    "the -o wide option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating a node failure by shutting down its network interface:\n",
    "\n",
    "    gcloud compute ssh gke-kubia-default-pool-b46381f1-zwko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sudo ifconfig eth0 down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you shut down the network interface, the ssh session will stop responding, so\n",
    "you need to open up another terminal or hard-exit from the ssh session. In the new\n",
    "terminal you can list the nodes to see if Kubernetes has detected that the node is\n",
    "down. This takes a minute or so. Then, the node’s status is shown as NotReady:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you list the pods now, you’ll still see the same three pods as before, because Kubernetes\n",
    "waits a while before rescheduling pods (in case the node is unreachable because\n",
    "of a temporary network glitch or because the Kubelet is restarting). If the node stays\n",
    "unreachable for several minutes, the status of the pods that were scheduled to that\n",
    "node changes to Unknown. At that point, the ReplicationController will immediately\n",
    "spin up a new pod. You can see this by listing the pods again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pods\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the age of the pods, you see that the kubia-dmdck pod is new. You again\n",
    "have three pod instances running, which means the ReplicationController has again\n",
    "done its job of bringing the actual state of the system to the desired state.\n",
    "The same thing happens if a node fails (either breaks down or becomes unreachable).\n",
    "No immediate human intervention is necessary. The system heals itself\n",
    "automatically.\n",
    "\n",
    "To bring the node back, you need to reset it with the following command:\n",
    "\n",
    "    gcloud compute instances reset gke-kubia-default-pool-b46381f1-zwko\n",
    "    \n",
    "When the node boots up again, its status should return to Ready, and the pod whose\n",
    "status was Unknown will be deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontally scaling pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve seen how ReplicationControllers make sure a specific number of pod instances\n",
    "is always running. Because it’s incredibly simple to change the desired number of replicas,\n",
    "this also means scaling pods horizontally is trivial.\n",
    "\n",
    "Scaling the number of pods up or down is as easy as changing the value of the replicas\n",
    "field in the ReplicationController resource. After the change, the Replication-\n",
    "Controller will either see too many pods exist (when scaling down) and delete part of\n",
    "them, or see too few of them (when scaling up) and create additional pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCALING UP A REPLICATIONCONTROLLER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your ReplicationController has been keeping three instances of your pod running.\n",
    "You’re going to scale that number up to 10 now. As you may remember, you’ve\n",
    "already scaled a ReplicationController in chapter 2. You could use the same command\n",
    "as before:\n",
    "    \n",
    "    kubectl scale rc kubia --replicas=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCALING A REPLICATIONCONTROLLER BY EDITING ITS DEFINITION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the kubectl scale command, you’re going to scale it in a declarative\n",
    "way by editing the ReplicationController’s definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl edit rc kubia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the text editor opens, find the spec.replicas field and change its value to 10,\n",
    "as shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you save the file and close the editor, the ReplicationController is updated and\n",
    "it immediately scales the number of pods to 10:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go. If the kubectl scale command makes it look as though you’re telling\n",
    "Kubernetes exactly what to do, it’s now much clearer that you’re making a declarative\n",
    "change to the desired state of the ReplicationController and not telling Kubernetes to\n",
    "do something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SCALING DOWN WITH THE KUBECTL SCALE COMMAND**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now scale back down to 3. You can use the kubectl scale command:\n",
    "    \n",
    "    kubectl scale rc kubia --replicas=3\n",
    "\n",
    "All this command does is modify the spec.replicas field of the ReplicationController’s\n",
    "definition—like when you changed it through kubectl edit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UNDERSTANDING THE DECLARATIVE APPROACH TO SCALING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizontally scaling pods in Kubernetes is a matter of stating your desire: “I want to\n",
    "have x number of instances running.” You’re not telling Kubernetes what or how to do\n",
    "it. You’re just specifying the desired state.\n",
    "\n",
    "This declarative approach makes interacting with a Kubernetes cluster easy. Imagine\n",
    "if you had to manually determine the current number of running instances and\n",
    "then explicitly tell Kubernetes how many additional instances to run. That’s more\n",
    "work and is much more error-prone. Changing a simple number is much easier, and\n",
    "in chapter 15, you’ll learn that even that can be done by Kubernetes itself if you\n",
    "enable horizontal pod auto-scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting a ReplicationController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you delete a ReplicationController through kubectl delete, the pods are also\n",
    "deleted. But because pods created by a ReplicationController aren’t an integral part\n",
    "of the ReplicationController, and are only managed by it, you can delete only the\n",
    "ReplicationController and leave the pods running, as shown in figure 4.7.\n",
    "\n",
    "This may be useful when you initially have a set of pods managed by a Replication-\n",
    "Controller, and then decide to replace the ReplicationController with a ReplicaSet,\n",
    "for example (you’ll learn about them next.). You can do this without affecting the\n",
    "pods and keep them running without interruption while you replace the Replication-\n",
    "Controller that manages them.\n",
    "\n",
    "When deleting a ReplicationController with kubectl delete, you can keep its\n",
    "pods running by passing the --cascade=false option to the command. Try that now:\n",
    "\n",
    "    $ kubectl delete rc kubia --cascade=false\n",
    "\n",
    "\n",
    "You’ve deleted the ReplicationController so the pods are on their own. They are no\n",
    "longer managed. But you can always create a new ReplicationController with the\n",
    "proper label selector and make them managed again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ReplicaSets instead of ReplicationControllers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, ReplicationControllers were the only Kubernetes component for replicating\n",
    "pods and rescheduling them when nodes failed. Later, a similar resource called a\n",
    "ReplicaSet was introduced. It’s a new generation of ReplicationController and\n",
    "replaces it completely (ReplicationControllers will eventually be deprecated).\n",
    "\n",
    "You could have started this chapter by creating a ReplicaSet instead of a Replication-\n",
    "Controller, but I felt it would be a good idea to start with what was initially available in\n",
    "Kubernetes. Plus, you’ll still see ReplicationControllers used in the wild, so it’s good\n",
    "for you to know about them. That said, you should always create ReplicaSets instead\n",
    "of ReplicationControllers from now on. They’re almost identical, so you shouldn’t\n",
    "have any trouble using them instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You usually won’t create them directly, but instead have them created automatically\n",
    "when you create the higher-level Deployment resource, which you’ll learn about\n",
    "in chapter 9. In any case, you should understand ReplicaSets, so let’s see how they differ\n",
    "from ReplicationControllers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing a ReplicaSet to a ReplicationController\n",
    "\n",
    "A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive\n",
    "pod selectors. Whereas a ReplicationController’s label selector only allows matching\n",
    "pods that include a certain label, a ReplicaSet’s selector also allows matching pods\n",
    "that lack a certain label or pods that include a certain label key, regardless of\n",
    "its value.\n",
    "\n",
    "Also, for example, a single ReplicationController can’t match pods with the label\n",
    "env=production and those with the label env=devel at the same time. It can only match\n",
    "either pods with the env=production label or pods with the env=devel label. But a single\n",
    "ReplicaSet can match both sets of pods and treat them as a single group.\n",
    "\n",
    "Similarly, a ReplicationController can’t match pods based merely on the presence\n",
    "of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica-\n",
    "Set can match all pods that include a label with the key env, whatever its actual value is\n",
    "(you can think of it as env=*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a ReplicaSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re going to create a ReplicaSet now to see how the orphaned pods that were created\n",
    "by your ReplicationController and then abandoned earlier can now be adopted\n",
    "by a ReplicaSet. First, you’ll rewrite your ReplicationController into a ReplicaSet by\n",
    "creating a new file called kubia-replicaset.yaml with the contents in the following\n",
    "listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl apply -f kubia-replicaset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: apps/v1\n",
    "kind: ReplicaSet\n",
    "metadata:\n",
    "  name: kubia\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: kubia\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: kubia\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: kubia\n",
    "        image: luksa/kubia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re creating are source of type ReplicaSet which has much the same contents as the ReplicationController you created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference is in the selector. Instead of listing labels the pods need to\n",
    "have directly under the selector property, you’re specifying them under selector\n",
    ".matchLabels. This is the simpler (and less expressive) way of defining label selectors\n",
    "in a ReplicaSet. Later, you’ll look at the more expressive option, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you still have three pods matching the app=kubia selector running from earlier,\n",
    "creating this ReplicaSet will not cause any new pods to be created. The ReplicaSet\n",
    "will take those existing three pods under its wing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and examining a ReplicaSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the ReplicaSet from the YAML file with the kubectl create command. After\n",
    "that, you can examine the ReplicaSet with kubectl get and kubectl describe:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TIP Use rs shorthand, which stands for replicaset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl describe rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the ReplicaSet isn’t any different from a ReplicationController. It’s\n",
    "showing it has three replicas matching the selector. If you list all the pods, you’ll see\n",
    "they’re still the same three pods you had before. The ReplicaSet didn’t create any new\n",
    "ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a quick introduction to ReplicaSets as an alternative to ReplicationControllers.\n",
    "Remember, always use them instead of ReplicationControllers, but you may still find\n",
    "ReplicationControllers in other people’s deployments.\n",
    "Now, delete the ReplicaSet to clean up your cluster a little. You can delete the\n",
    "ReplicaSet the same way you’d delete a ReplicationController:\n",
    "    \n",
    "    kubectl delete rs kubia\n",
    "\n",
    "  \n",
    "Deleting the ReplicaSet should delete all the pods. List the pods to confirm that’s\n",
    "the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running exactly one pod on each node with DaemonSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both ReplicationControllers and ReplicaSets are used for running a specific number of pods deployed anywhere in the Kubernetes cluster. But certain cases exist when you want a pod to run on each and every node in the cluster (and each node needs to run exactly one instance of the pod, as shown in figure 4.8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those cases include infrastructure-related pods that perform system-level operations. For example, you’ll want to run a log collector and a resource monitor on every node. Another good example is Kubernetes’ own kube-proxy process, which needs to run on all nodes to make services work.\n",
    "\n",
    "Outside of Kubernetes, such processes would usually be started through system init scripts or the systemd daemon during node boot up. On Kubernetes nodes, you can still use systemd to run your system processes, but then you can’t take advantage of all the features Kubernetes provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a DaemonSet to run a pod on every node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a pod on all cluster nodes, you create a DaemonSet object, which is much like a ReplicationController or a ReplicaSet, except that pods created by a DaemonSet already have a target node specified and skip the Kubernetes Scheduler. They aren’t scattered around the cluster randomly.\n",
    "\n",
    "A DaemonSet makes sure it creates as many pods as there are nodes and deploys each one on its own node, as shown in figure 4.8.\n",
    "\n",
    "Whereas a ReplicaSet (or ReplicationController) makes sure that a desired number of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a desired replica count. It doesn’t need it because its job is to ensure that a pod matching its pod selector is running on each node.\n",
    "\n",
    "If a node goes down, the DaemonSet doesn’t cause the pod to be created elsewhere. But when a new node is added to the cluster, the DaemonSet immediately deploys a new pod instance to it. It also does the same if someone inadvertently deletes one of the pods, leaving the node without the DaemonSet’s pod. Like a ReplicaSet, a DaemonSet creates the pod from the pod template configured in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a DaemonSet to run pods only on certain nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DaemonSet deploys pods to all nodes in the cluster, unless you specify that the pods should only run on a subset of all the nodes. This is done by specifying the node-Selector property in the pod template, which is part of the DaemonSet definition (similar to the pod template in a ReplicaSet or ReplicationController).\n",
    "\n",
    "You’ve already used node selectors to deploy a pod onto specific nodes in chapter 3. A node selector in a DaemonSet is similar—it defines the nodes the DaemonSet must deploy its pods to.\n",
    "\n",
    "> Later in the book, you’ll learn that nodes can be made unschedulable, preventing pods from being deployed to them. A DaemonSet will deploy pods even to such nodes, because the unschedulable attribute is only used by the Scheduler, whereas pods managed by a DaemonSet bypass the Scheduler completely. This is usually desirable, because DaemonSets are meant to run system services, which usually need to run even on unschedulable nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explaining DaemonSets with an example**\n",
    "\n",
    "Let’s imagine having a daemon called ssd-monitor that needs to run on all nodes that contain a solid-state drive (SSD). You’ll create a DaemonSet that runs this daemon on all nodes that are marked as having an SSD. The cluster administrators have added the disk=ssd label to all such nodes, so you’ll create the DaemonSet with a node selector that only selects nodes with that label, as shown in figure 4.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a DaemonSet YAML definition**\n",
    "\n",
    "You’ll create a DaemonSet that runs a mock ssd-monitor process, which prints “SSD OK” to the standard output every five seconds. I’ve already prepared the mock container image and pushed it to Docker Hub, so you can use it instead of building your own. Create the YAML for the DaemonSet, as shown in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: ssd-monitor\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ssd-monitor\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ssd-monitor\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        disk: ssd\n",
    "      containers:\n",
    "      - name: main\n",
    "        image: luksa/ssd-monitor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’re defining a DaemonSet that will run a pod with a single container based on the luksa/ssd-monitor container image. An instance of this pod will be created for each node that has the disk=ssd label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating the DaemonSet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll create the DaemonSet like you always create resources from a YAML file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl apply -f ssd-monitor-daemonset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see the created DaemonSet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those zeroes look strange. Didn’t the DaemonSet deploy any pods? List the pods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get po"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the pods? Do you know what’s going on? Yes, you forgot to label your nodes with the disk=ssd label. No problem—you can do that now. The DaemonSet should detect that the nodes’ labels have changed and deploy the pod to all nodes with a matching label. Let’s see if that’s true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding the required label to your node(s)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless if you’re using Minikube, GKE, or another multi-node cluster, you’ll need to list the nodes first, because you’ll need to know the node’s name when labeling it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get node\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add the disk=ssd label to one of your nodes like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl label node minikube disk=ssd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DaemonSet should have created one pod now. Let’s see:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get po"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay; so far so good. If you have multiple nodes and you add the same label to further nodes, you’ll see the DaemonSet spin up pods for each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing the required label from the node**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine you’ve made a mistake and have mislabeled one of the nodes. It has a spinning disk drive, not an SSD. What happens if you change the node’s label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     kubectl label node minikube disk=hdd --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see if the change has any effect on the pod that was running on that node:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get po"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pod is being terminated. But you knew that was going to happen, right? This wraps up your exploration of DaemonSets, so you may want to delete your ssd-monitor DaemonSet. If you still have any other daemon pods running, you’ll see that deleting the DaemonSet deletes those pods as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl delete ds ssd-monitor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running pods that perform a single completable task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we’ve only talked about pods than need to run continuously. You’ll have cases where you only want to run a task that terminates after completing its work. ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are never considered completed. Processes in such pods are restarted when they exit. But in a completable task, after its process terminates, it should not be restarted again.\n",
    "\n",
    "### Introducing the Job resource\n",
    "Kubernetes includes support for this through the Job resource, which is similar to the other resources we’ve discussed in this chapter, but it allows you to run a pod whose container isn’t restarted when the process running inside finishes successfully. Once it does, the pod is considered complete.\n",
    "\n",
    "In the event of a node failure, the pods on that node that are managed by a Job will be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of the process itself (when the process returns an error exit code), the Job can be configured to either restart the container or not.\n",
    "\n",
    "Figure 4.10 shows how a pod created by a Job is rescheduled to a new node if the node it was initially scheduled to fails. The figure also shows both a managed pod, which isn’t rescheduled, and a pod backed by a ReplicaSet, which is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task finishes properly. You could run the task in an unmanaged pod and wait for it to finish, but in the event of a node failing or the pod being evicted from the node while it is performing its task, you’d need to manually recreate it. Doing this manually doesn’t make sense—especially if the job takes hours to complete.\n",
    "\n",
    "An example of such a job would be if you had data stored somewhere and you needed to transform and export it somewhere. You’re going to emulate this by running a container image built on top of the busybox image, which invokes the sleep command for two minutes. I’ve already built the image and pushed it to Docker Hub, but you can peek into its Dockerfile in the book’s code archive.\n",
    "\n",
    "###  Defining a Job resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Job manifest as in the following listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: batch-job\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: batch-job\n",
    "    spec:\n",
    "      restartPolicy: OnFailure\n",
    "      containers:\n",
    "      - name: main\n",
    "        image: luksa/batch-job\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs are part of the batch API group and v1 API version. The YAML defines a resource of type Job that will run the luksa/batch-job image, which invokes a process that runs for exactly 120 seconds and then exits.\n",
    "\n",
    "In a pod’s specification, you can specify what Kubernetes should do when the processes running in the container finish. This is done through the restartPolicy pod spec property, which defaults to Always. Job pods can’t use the default policy, because they’re not meant to run indefinitely. Therefore, you need to explicitly set the restart policy to either OnFailure or Never. This setting is what prevents the container from being restarted when it finishes (not the fact that the pod is being managed by a Job resource)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl apply -f ex05-exporter.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing a Job run a pod\n",
    "\n",
    "After you create this Job with the kubectl apply command, you should see it start up a pod immediately:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     kubectl get jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get po"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the two minutes have passed, the pod will no longer show up in the pod list and the Job will be marked as completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the pod isn’t deleted when it completes is to allow you to examine its logs; for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl logs batch-job-28qf4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pod will be deleted when you delete it or the Job that created it. Before you do that, let’s look at the Job resource again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Job is shown as having completed successfully. But why is that piece of information shown as a number instead of as yes or true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running multiple pod instances in a Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs may be configured to create more than one pod instance and run them in parallel or sequentially. This is done by setting the completions and the parallelism properties in the Job spec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
