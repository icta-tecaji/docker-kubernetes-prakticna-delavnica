{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single host networking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viri:\n",
    "- [Networking overview](https://docs.docker.com/network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker container networking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A container attached to a Docker network will get a unique IP address that is routable\n",
    "from other containers attached to the same Docker network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with this approach is that there is no easy way for any software\n",
    "running inside a container to determine the IP address of the host where the container\n",
    "is running. This inhibits a container from advertising its service endpoint to\n",
    "other services outside the container network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker’s networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality:\n",
    "- **bridge**: The default network driver. If you don’t specify a driver, this is the type of network you are creating. Bridge networks are usually used when your applications run in standalone containers that need to communicate. *User-defined bridge networks are best when you need multiple containers to communicate on the same Docker host.*\n",
    "- **host**: For standalone containers, remove network isolation between the container and the Docker host, and use the host’s networking directly. host is only available for swarm services on Docker 17.06 and higher. *Host networks are best when the network stack should not be isolated from the Docker host, but you want other aspects of the container to be isolated.*\n",
    "- **overlay**: Overlay networks connect multiple Docker daemons together and enable swarm services to communicate with each other. You can also use overlay networks to facilitate communication between a swarm service and a standalone container, or between two standalone containers on different Docker daemons. This strategy removes the need to do OS-level routing between these containers. *Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using swarm services.*\n",
    "- **macvlan**: Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses. Using the macvlan driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host’s network stack.\n",
    "- **none**: For this container, disable all networking. Usually used in conjunction with a custom network driver. none is not available for swarm services. \n",
    "- **Network plugins**: You can install and use third-party network plugins with Docker. These plugins are available from Docker Hub or from third-party vendors. See the vendor’s documentation for installing and using a given network plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of networking, a bridge network is a Link Layer device which forwards traffic between network segments. A bridge can be a hardware device or a software device running within a host machine’s kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Docker bridge network driver uses Linux namespaces, virtual Ethernet devices,\n",
    "and the Linux firewall to build a specific and customizable virtual network topology\n",
    "called a bridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Containers have their own private loopback interface and a separate virtual Ethernet\n",
    "interface linked to another virtual interface in the host’s namespace. These two\n",
    "linked interfaces form a link between the host’s network and the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bridge networks apply to containers running on the same Docker daemon host.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start Docker, a default bridge network (also called bridge) is created automatically, and newly-started containers connect to it unless otherwise specified. You can also create user-defined custom bridge networks. User-defined bridge networks are superior to the default bridge network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like\n",
    "typical home networks, each container is assigned a unique private IP address that’s\n",
    "not directly reachable from the external network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connections are routed through\n",
    "another Docker network that routes traffic between containers and may connect to\n",
    "the host’s network to form a bridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Containers connected to the same user-defined bridge network effectively expose all ports to each other.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between user-defined bridges and the default bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **User-defined bridges provide automatic DNS resolution between containers**: Containers on the default bridge network can only access each other by IP addresses, unless you use the --link option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias.\n",
    "- **User-defined bridges provide better isolation**: All containers without a --network specified, are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate. Using a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.\n",
    "- **Containers can be attached and detached from user-defined networks on the fly**: During a container’s lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.\n",
    "- **Each user-defined network creates a configurable bridge**: User-defined bridge networks are created and configured using docker network create. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.\n",
    "- **Linked containers on the default bridge network share environment variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage a user-defined bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the docker `network create command` to create a user-defined bridge network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker network create my-net\n",
    "    docker network ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the subnet, the IP address range, the gateway, and other options. See the docker network create reference or the output of docker network create --help for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [docker network create](https://docs.docker.com/engine/reference/commandline/network_create/#specify-advanced-options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the docker network rm command to remove a user-defined bridge network. If containers are currently connected to the network, disconnect them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker network rm my-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker network create \\\n",
    "        --driver bridge \\\n",
    "        --label project=dockerinaction \\\n",
    "        --label chapter=5 \\\n",
    "        --attachable \\\n",
    "        --scope local \\\n",
    "        --subnet 10.0.42.0/24 \\\n",
    "        --ip-range 10.0.42.128/25 \\\n",
    "        user-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command creates a new local bridge network named user-network. Adding\n",
    "label metadata to the network will help in identifying the resource later. Marking the\n",
    "new network as attachable allows you to attach and detach containers to the network\n",
    "at any time. Here you’ve manually specified the network scope property and set it to\n",
    "the default value for this driver. Finally, a custom subnet and assignable address range\n",
    "was defined for this network, 10.0.42.0/24, assigning from the upper half of the last\n",
    "octet (10.0.42.128/25). This means that as you add containers to this network, they\n",
    "will receive IP addresses in the range from 10.0.42.128 to 10.0.42.255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a bridge network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re going to run network software inside a container on a container network,\n",
    "you should have a solid understanding of what that network looks like from within\n",
    "a container. Start exploring your new bridge network by creating a new container\n",
    "attached to that network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker run -it \\\n",
    "        --network user-network \\\n",
    "        --name network-explorer \\\n",
    "        alpine:3.8 \\\n",
    "        sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the IPv4 addresses available in the container from your terminal (which is\n",
    "now attached to the running container) by running the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ip -f inet -4 -o addr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like this:\n",
    "\n",
    "    1: lo inet 127.0.0.1/8 scope host lo\\ ...\n",
    "    18: eth0 inet 10.0.42.129/24 brd 10.0.42.255 scope global eth0\\ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from this list that the container has two network devices with IPv4\n",
    "addresses. Those are the loopback interface (or localhost) and eth0 (a virtual Ethernet\n",
    "device), which is connected to the bridge network. Further, you can see that eth0\n",
    "has an IP address within the range and subnet specified by the user-network configuration\n",
    "(the range from 10.0.42.128 to 10.0.42.255). That IP address is the one that any\n",
    "other container on this bridge network would use to communicate with services you\n",
    "run in this container. The loopback interface can be used only for communication\n",
    "within the same container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create another bridge network and attach your running network-explorer\n",
    "container to both networks. First, detach your terminal from the running container\n",
    "(press Ctrl-P and then Ctrl-Q) and then create the second bridge network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker network create \\\n",
    "        --driver bridge \\\n",
    "        --label project=dockerinaction \\\n",
    "        --label chapter=5 \\\n",
    "        --attachable \\\n",
    "        --scope local \\\n",
    "        --subnet 10.0.43.0/24 \\\n",
    "        --ip-range 10.0.43.128/25 \\\n",
    "        user-network2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the second network has been created, you can attach the network-explorer\n",
    "container (still running):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker network connect user-network2 network-explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the container has been attached to the second network, reattach your terminal\n",
    "to continue your exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker attach network-explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back in the container, examining the network interface configuration again will\n",
    "show something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ip -f inet -4 -o addr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1: lo inet 127.0.0.1/8 scope host lo\\ ...\n",
    "    18: eth0 inet 10.0.42.129/24 brd 10.0.42.255 scope global eth0\\ ...\n",
    "    20: eth1 inet 10.0.43.129/24 brd 10.0.43.255 scope global eth1\\ ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, this output shows that the network-explorer container is\n",
    "attached to both user-defined bridge networks.\n",
    "\n",
    "Networking is all about communication between multiple parties, and examining\n",
    "a network with only one running container can be a bit boring. But is there anything\n",
    "else attached to a bridge network by default? Another tool is needed to\n",
    "continue exploring. Install the nmap package inside your running container by using\n",
    "this command:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    apk update && apk add nmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nmap is a powerful network inspection tool that can be used to scan network address\n",
    "ranges for running machines, fingerprint those machines, and determine what services\n",
    "they are running. For our purposes, we simply want to determine what other containers\n",
    "or other network devices are available on our bridge network. Run the following command\n",
    "to scan the 10.0.42.0/24 subnet that we defined for our bridge network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nmap -sn 10.0.42.* -sn 10.0.43.* -oG /dev/stdout | grep Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that only two devices are attached to each of the bridge networks: the gateway\n",
    "adapters created by the bridge network driver and the currently running container.\n",
    "Create another container on one of the two bridge networks for more interesting results.\n",
    "Detach from the terminal again (Ctrl-P, Ctrl-Q) and start another container attached\n",
    "to user-network2. Run the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker run -d \\\n",
    "        --name lighthouse \\\n",
    "        --network user-network2 \\\n",
    "        alpine:3.8 \\\n",
    "        sleep 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the lighthouse container has started, reattach to your network-explorer\n",
    "container:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker attach network-explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from the shell in that container, run the network scan again. The results show\n",
    "that the lighthouse container is up and running, and accessible from the networkexplorer\n",
    "container via its attachment to user-network2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nmap -sn 10.0.42.* -sn 10.0.43.* -oG /dev/stdout | grep Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discovering the lighthouse container on the network confirms that the network\n",
    "attachment works as expected, and demonstrates how the DNS-based service discovery\n",
    "system works. When you scanned the network, you discovered the new node by\n",
    "its IP address, and nmap was able to resolve that IP address to a name. This means\n",
    "that you (or your code) can discover individual containers on the network based on\n",
    "their name. Try this yourself by running nslookup lighthouse inside the container.\n",
    "Container hostnames are based on the container name, or can be set manually at container\n",
    "creation time by specifying the --hostname flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nslookup lighthouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pokažemo, da se lahko kontejnerji v povezanih omrežjih najdejo po imenu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ping lighthouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker rm -f lighthouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker network ls\n",
    "    docker network rm user-network user-network2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling inbound traffic with NodePort publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker container networks are all about simple connectivity and routing between\n",
    "containers. Connecting services running in those containers with external network clients\n",
    "requires an extra step. Since container networks are connected to the broader\n",
    "network via network address translation, you have to specifically tell Docker how to\n",
    "forward traffic from the external network interfaces. You need to specify a TCP or\n",
    "UDP port on the host interface and a target container and container port, similar to\n",
    "forwarding traffic through a NAT barrier on your home network.\n",
    "\n",
    "NodePort publishing is a term we’ve used here to match Docker and other ecosystem\n",
    "projects. The Node portion is an inference to the host as typically a node in a larger\n",
    "cluster of machines.\n",
    "\n",
    "Port publication configuration is provided at container creation time and cannot be\n",
    "changed later. The docker run and docker create commands provide a -p or --publish\n",
    "list option. Like other options, the -p option takes a colon-delimited string argument.\n",
    "That argument specifies the host interface, the port on the host to forward, the target\n",
    "port, and the port protocol. All of the following arguments are equivalent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 0.0.0.0:8080:8080/tcp\n",
    "- 8080:8080/tcp\n",
    "- 8080:8080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of those options will forward TCP port 8080 from all host interfaces to TCP port\n",
    "8080 in the new container. The first argument is the full form. To put the syntax in a\n",
    "more complete context, consider the following example commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker run --rm -p 8088:8080/tcp alpine:3.8 echo \"host UDP 8088 -> container UDP 8080\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('3.9.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "49295d7be20e15a65a7ead1eee80289bbf09f3482fe4d303cdf9f84b66666c7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
