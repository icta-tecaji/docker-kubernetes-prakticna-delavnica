{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing pods with labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have two pods running in your cluster. When deploying actual\n",
    "applications, most users will end up running many more pods. As the number of\n",
    "pods increases, the need for categorizing them into subsets becomes more and\n",
    "more evident.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with microservices architectures, the number of deployed microservices\n",
    "can easily exceed 20 or more. Those components will probably be replicated\n",
    "(multiple copies of the same component will be deployed) and multiple versions or\n",
    "releases (stable, beta, canary, and so on) will run concurrently. This can lead to hundreds\n",
    "of pods in the system. Without a mechanism for organizing them, you end up\n",
    "with a big, incomprehensible mess, such as the one shown in figure 3.6. The figure\n",
    "shows pods of multiple microservices, with several running multiple replicas, and others\n",
    "running different releases of the same microservice.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s evident you need a way of organizing them into smaller groups based on arbitrary\n",
    "criteria, so every developer and system administrator dealing with your system can easily\n",
    "see which pod is which. And you’ll want to operate on every pod belonging to a certain\n",
    "group with a single action instead of having to perform the action for each pod\n",
    "individually.\n",
    "\n",
    "**Organizing pods and all other Kubernetes objects is done through labels.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are a simple, yet incredibly powerful, Kubernetes feature for organizing not\n",
    "only pods, but all other Kubernetes resources. A label is an **arbitrary key-value pair** you\n",
    "attach to a resource, which is then utilized when selecting resources using label selectors\n",
    "(resources are filtered based on whether they include the label specified in the selector).\n",
    "A resource can have more than one label, as long as the keys of those labels are\n",
    "unique within that resource. You usually attach labels to resources when you create\n",
    "them, but you can also add additional labels or even modify the values of existing\n",
    "labels later without having to recreate the resource."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s turn back to the microservices example from figure 3.6. By adding labels to\n",
    "those pods, you get a much-better-organized system that everyone can easily make\n",
    "sense of. Each pod is labeled with two labels:\n",
    "- app, which specifies which app, component, or microservice the pod belongs to.\n",
    "- rel, which shows whether the application running in the pod is a stable, beta,\n",
    "or a canary release."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DEFINITION A canary release is when you deploy a new version of an application\n",
    "next to the stable version, and only let a small fraction of users hit the\n",
    "new version to see how it behaves before rolling it out to all users. This prevents\n",
    "bad releases from being exposed to too many users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By adding these two labels, you’ve essentially organized your pods into two dimensions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every developer or ops person with access to your cluster can now easily see the system’s\n",
    "structure and where each pod fits in by looking at the pod’s labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying labels when creating a pod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you’ll see labels in action by creating a new pod with two labels. Create a new file\n",
    "called kubia-manual-with-labels.yaml with the contents of the following listing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kubia-manual-v2\n",
    "  labels:\n",
    "    creation_method: manual\n",
    "    env: prod\n",
    "spec:\n",
    "  containers:\n",
    "   - image: luksa/kubia\n",
    "     name: kubia\n",
    "     ports:\n",
    "     - containerPort: 8080\n",
    "       protocol: TCP\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ve included the labels creation_method=manual and env=data.labels section.\n",
    "You’ll create this pod now:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl apply -f ex02-kubia-manual-with-labels.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kubectl get pods command doesn’t list any labels by default, but you can see\n",
    "them by using the --show-labels switch:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pod --show-labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of listing all labels, if you’re only interested in certain labels, you can specify\n",
    "them with the -L switch and have each displayed in its own column. List pods again\n",
    "and show the columns for the two labels you’ve attached to your kubia-manual-v2 pod:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl get pod -L creation_method,env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying labels of existing pods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels can also be added to and modified on existing pods. Because the kubia-manual\n",
    "pod was also created manually, let’s add the creation_method=manual label to it:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl label pod kubia-manual creation_method=manual"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s also change the env=prod label to env=debug on the kubia-manual-v2 pod,\n",
    "to see how existing labels can be changed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE You need to use the --overwrite option when changing existing labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kubectl label po kubia-manual-v2 env=debug --overwrite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the pods again to see the updated labels:\n",
    "    \n",
    "    kubectl get po -L creation_method,env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, attaching labels to resources is trivial, and so is changing them on\n",
    "existing resources. It may not be evident right now, but this is an incredibly powerful\n",
    "feature, as you’ll see in the next chapter. But first, let’s see what you can do with these\n",
    "labels, in addition to displaying them when listing pods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing subsets of pods through label selectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attaching labels to resources so you can see the labels next to each resource when listing\n",
    "them isn’t that interesting. But labels go hand in hand with label selectors. Label\n",
    "selectors allow you to select a subset of pods tagged with certain labels and perform an\n",
    "operation on those pods. A label selector is a criterion, which filters resources based\n",
    "on whether they include a certain label with a certain value.\n",
    "A label selector can select resources based on whether the resource\n",
    "- Contains (or doesn’t contain) a label with a certain key\n",
    "- Contains a label with a certain key and value\n",
    "- Contains a label with a certain key, but with a value not equal to the one you\n",
    "specify"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing pods using a label selector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use label selectors on the pods you’ve created so far. To see all pods you created\n",
    "manually (you labeled them with creation_method=manual), do the following:\n",
    "\n",
    "    kubectl get po -l creation_method=manual\n",
    "    \n",
    "To list all pods that include the env label, whatever its value is:\n",
    "\n",
    "    kubectl get po -l env\n",
    "\n",
    "And those that don’t have the env label:\n",
    "\n",
    "    kubectl get po -l '!env'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE Make sure to use single quotes around !env, so the bash shell doesn’t\n",
    "evaluate the exclamation mark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using labels and selectors to constrain pod scheduling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the pods you’ve created so far have been scheduled pretty much randomly across\n",
    "your worker nodes. As I’ve mentioned in the previous chapter, this is the proper way\n",
    "of working in a Kubernetes cluster. Because Kubernetes exposes all the nodes in the\n",
    "cluster as a single, large deployment platform, it shouldn’t matter to you what node a\n",
    "pod is scheduled to. Because each pod gets the exact amount of computational\n",
    "resources it requests (CPU, memory, and so on) and its accessibility from other pods\n",
    "isn’t at all affected by the node the pod is scheduled to, usually there shouldn’t be any\n",
    "need for you to tell Kubernetes exactly where to schedule your pods.\n",
    "\n",
    "Certain cases exist, however, where you’ll want to have at least a little say in where\n",
    "a pod should be scheduled. A good example is when your hardware infrastructure\n",
    "isn’t homogenous. If part of your worker nodes have spinning hard drives, whereas\n",
    "others have SSDs, you may want to schedule certain pods to one group of nodes and\n",
    "the rest to the other. Another example is when you need to schedule pods performing\n",
    "intensive GPU-based computation only to nodes that provide the required GPU\n",
    "acceleration.\n",
    "\n",
    "You never want to say specifically what node a pod should be scheduled to, because\n",
    "that would couple the application to the infrastructure, whereas the whole idea of\n",
    "Kubernetes is hiding the actual infrastructure from the apps that run on it. But if you\n",
    "want to have a say in where a pod should be scheduled, instead of specifying an exact\n",
    "node, you should describe the node requirements and then let Kubernetes select a\n",
    "node that matches those requirements. This can be done through node labels and\n",
    "node label selectors.\n",
    "\n",
    "### Scheduling to one specific node\n",
    "\n",
    "Similarly, you could also schedule a pod to an exact node, because each node also has\n",
    "a unique label with the key kubernetes.io/hostname and value set to the actual hostname\n",
    "of the node. But setting the nodeSelector to a specific node by the hostname\n",
    "label may lead to the pod being unschedulable if the node is offline. You shouldn’t\n",
    "think in terms of individual nodes. Always think about logical groups of nodes that satisfy\n",
    "certain criteria specified through label selectors.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('3.9.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "49295d7be20e15a65a7ead1eee80289bbf09f3482fe4d303cdf9f84b66666c7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
